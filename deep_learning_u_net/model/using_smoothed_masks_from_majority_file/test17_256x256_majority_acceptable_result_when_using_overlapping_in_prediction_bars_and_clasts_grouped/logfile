# IPython log file

# Sat, 17 Jul 2021 19:06:32
path_global = '/media/jose/datos/deep_learning/geomorfologia-detalle-tramo-1km-rio-mana/deep_learning_u_net/'
import sys
sys.path.insert(0, path_global + 'scripts')
from simple_multi_unet_model import multi_unet_model #Uses softmax 
from keras.utils import normalize
import os
import glob
import gc
import cv2
import numpy as np
from matplotlib import pyplot as plt
import crop_patches as cp
from unpad import unpad
from random import sample, seed #For randomly select image files
import hashlib #To generate md5sum hash of image files
from sklearn.model_selection import train_test_split #To split the data in train and test sets
from keras.utils import to_categorical #To convert train masks to categorical
import segmentation_models as sm #To define some loss-functions
from keras.models import load_model #To load saved models
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger #To add checkpoints and logger in CSV format
import pandas as pd #To convert the history object to Data Frame
import pickle #To save the history object
# Sat, 17 Jul 2021 19:06:54
SIZE_X = 256
SIZE_Y = 256
n_classes=7 #Number of classes for segmentation
# Sat, 17 Jul 2021 19:06:59
train_images = []
ti_list=[]
for directory_path in glob.glob(path_global + "images/train/tiles/"):
    for img_path in glob.glob(os.path.join(directory_path, "*.tif")):
        ti_list.append(img_path) #This is a list of names
        img = cv2.imread(img_path)
        train_images.append(img) #This is a list of arrays
#Convert list to array for processing
train_images = np.array(train_images)
# Sat, 17 Jul 2021 19:07:31
ti_names=[] #List of train-image names
for i in ti_list: j=str.split(i, '/'); ti_names.append(j[-1])
seed(0)
sample(ti_names, 10)
hash_trains = []
seed(0)
for i in sample(ti_names, 10):
    hs = hashlib.md5(str(path_global + "images/train/masks/" + i).encode('utf-8')).hexdigest()
    hash_trains.append(i + '   ' + hs)
hash_trains
#[Out]# ['tile_1_2_2_0.tif   a2d67d0b4376801f4f7e8cc4a013c10e',
#[Out]#  'tile_12_26_1_4.tif   c866bfa144dd09e440784c5d7fa8a055',
#[Out]#  'tile_11_13_5_3.tif   3aacc0363cb8c4de7c2d2ca0238fbc1a',
#[Out]#  'tile_12_26_3_0.tif   99742ed332654b8f45eb07155c250fc9',
#[Out]#  'tile_14_20_1_3.tif   18781bd33b0e4d306fa7f075715947fd',
#[Out]#  'tile_3_4_1_0.tif   c3ee6a37e8319edad830a00e67289531',
#[Out]#  'tile_11_29_6_0.tif   af60afa9987f0075c9290e5224fb6f37',
#[Out]#  'tile_1_2_5_5.tif   263325386cfbc830c02e18a61107458f',
#[Out]#  'tile_7_8_3_0.tif   8c8d8dce5b6603081407dd275ca459e2',
#[Out]#  'tile_14_23_1_0.tif   a6e3b5424e4f77e200c78093f5d2f0d4']
# Sat, 17 Jul 2021 19:07:37
train_masks = []
tm_list = []
for directory_path in glob.glob(path_global + "images/train/masks/"):
    for mask_path in glob.glob(os.path.join(directory_path, "*.tif")):
        tm_list.append(mask_path)
        mask = cv2.imread(mask_path, 0)
        train_masks.append(mask)
#Convert list to array for processing
train_masks = np.array(train_masks)
# Sat, 17 Jul 2021 19:07:40
tm_names=[]
for l in tm_list: m=str.split(l, '/'); tm_names.append(m[-1])
ti_names==tm_names #If True, both images and mask names are consistently ordered
#[Out]# True
# Sat, 17 Jul 2021 19:07:45
hash_masks = []
seed(0)
for i in sample(tm_names, 10):
    hs = hashlib.md5(str(path_global + "images/train/masks/" + i).encode('utf-8')).hexdigest()
    hash_masks.append(i + '   ' + hs)
hash_masks
#[Out]# ['tile_1_2_2_0.tif   a2d67d0b4376801f4f7e8cc4a013c10e',
#[Out]#  'tile_12_26_1_4.tif   c866bfa144dd09e440784c5d7fa8a055',
#[Out]#  'tile_11_13_5_3.tif   3aacc0363cb8c4de7c2d2ca0238fbc1a',
#[Out]#  'tile_12_26_3_0.tif   99742ed332654b8f45eb07155c250fc9',
#[Out]#  'tile_14_20_1_3.tif   18781bd33b0e4d306fa7f075715947fd',
#[Out]#  'tile_3_4_1_0.tif   c3ee6a37e8319edad830a00e67289531',
#[Out]#  'tile_11_29_6_0.tif   af60afa9987f0075c9290e5224fb6f37',
#[Out]#  'tile_1_2_5_5.tif   263325386cfbc830c02e18a61107458f',
#[Out]#  'tile_7_8_3_0.tif   8c8d8dce5b6603081407dd275ca459e2',
#[Out]#  'tile_14_23_1_0.tif   a6e3b5424e4f77e200c78093f5d2f0d4']
# Sat, 17 Jul 2021 19:07:47
np.unique(train_masks)
#[Out]# array([0, 1, 2, 3, 4, 5, 6], dtype=uint8)
# Sat, 17 Jul 2021 19:07:50
train_images = normalize(train_images, axis=3)
# Sat, 17 Jul 2021 19:07:52
train_masks_input = np.expand_dims(train_masks, axis=3)
# Sat, 17 Jul 2021 19:08:01
X_train, X_test, y_train, y_test = train_test_split(train_images, train_masks_input, test_size = 0.25, random_state = 0)
# This code just below creates a corresponding list of file names reserved for testing
X1_ti_names, X_test_ti_names=train_test_split(np.array(ti_names), test_size=0.25, random_state=0)


###############################################
# Print again the class values in the training dataset (y_train)
print("Class values in the dataset are ... ", np.unique(y_train))  # 0 is the background/few unlabeled 

###############################################
# Convert train labels to categorical, otherwise, the model would treat the
# the dependent variable as numerical. Each class will be placed in a separated slice
y_train_cat = to_categorical(y_train, num_classes=n_classes)
# Do the same with the test labels
y_test_cat = to_categorical(y_test, num_classes=n_classes)
# Sat, 17 Jul 2021 19:08:10
IMG_HEIGHT = X_train.shape[1]
IMG_WIDTH  = X_train.shape[2]
IMG_CHANNELS = X_train.shape[3]

###############################################
# Define the model based on the multi_unet_model function in the script 
# "simple_multi_unet_model.py"
def get_model():
    return multi_unet_model(
        n_classes=n_classes,
        IMG_HEIGHT=IMG_HEIGHT,
        IMG_WIDTH=IMG_WIDTH,
        IMG_CHANNELS=IMG_CHANNELS)
model = get_model()
# Sat, 17 Jul 2021 19:08:13
model.compile(optimizer='adam', loss=sm.losses.CategoricalFocalLoss(), metrics=['accuracy'])
# Sat, 17 Jul 2021 19:08:15
model.summary()
# Sat, 17 Jul 2021 19:08:22
model_checkpoint_filepath= path_global + "model/model-epoch-{epoch:03d}-accuracy-{accuracy:03f}-val-accuracy-{val_accuracy:03f}-val-loss-{val_loss:03f}.hdf5" #File name includes epoch and validation accuracy.
#Use Mode = max for accuracy and min for loss. 
checkpoint = ModelCheckpoint(model_checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=False, mode='max', save_freq='epoch')
#CSVLogger logs epoch, acc, loss, val_acc, val_loss
log_csv = CSVLogger(path_global + 'model/my_logs.csv', separator=',', append=False)
callbacks_list = [checkpoint, log_csv]

#We can now use these generators to train our model. 
#Give this a name so we can call it later for plotting loss, accuracy etc. as a function of epochs.
# history = model.fit_generator(
#         X_train, y_train_cat, 
#         steps_per_epoch=2000 // 16,    #The 2 slashes division return rounded integer
#         epochs=5,
#         validation_data=(X_test, y_test_cat), 
#         validation_steps=800 // 16,
#         callbacks=callbacks_list)
# Sat, 17 Jul 2021 19:08:29
history = model.fit(X_train, y_train_cat,
                    batch_size = 16, 
                    verbose=1, 
                    epochs=100,
                    validation_data=(X_test, y_test_cat), 
                    # class_weight=class_weights,
                    # sample_weight= class_weights_arr,
                    shuffle=False,
                    callbacks=callbacks_list)
os.system('echo "Job finished" | mail -s "Job finished" zoneminderjr@gmail.com') #Yes, this is THE actual computation
#[Out]# 0
# Sat, 17 Jul 2021 22:13:13
history_df = pd.DataFrame(history.history)
pickle.dump(history.history, open(path_global + 'model/history', 'wb'))
# Sat, 17 Jul 2021 22:15:56
model=load_model('/media/jose/datos/deep_learning/geomorfologia-detalle-tramo-1km-rio-mana/deep_learning_u_net/model/using_smoothed_masks_from_majority_file/test17_256x256_majority_acceptable_result_when_using_overlapping_in_prediction_bars_and_clasts_grouped/model-epoch-096-accuracy-0.952686-val-accuracy-0.928995-val-loss-0.003136.hdf5', custom_objects={'focal_loss': sm.losses.CategoricalFocalLoss()}) #!!!!!!GOOD MODEL
# Sat, 17 Jul 2021 22:15:59
_, acc = model.evaluate(X_test, y_test_cat)
print("Accuracy is = ", (acc * 100.0), "%")
# Sat, 17 Jul 2021 22:16:10
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
# Sat, 17 Jul 2021 22:16:22
y_pred=model.predict(X_test)
y_pred_argmax=np.argmax(y_pred, axis=3)

##################################################

#Using built in keras function
from keras.metrics import MeanIoU
# n_classes = 7
IOU_keras = MeanIoU(num_classes=n_classes)  
IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)
print("Mean IoU =", IOU_keras.result().numpy())


#To calculate I0U for each class...
values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)
print(values)
# class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[0,4] + values[0,5] + values[0,6] + values[0,7] + values[1,0] + values[2,0] + values[3,0] + values[4,0] + values[5,0] + values[6,0] + values[7,0])
# class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[1,4] + values[1,5] + values[1,6] + values[1,7] + values[0,1] + values[2,1] + values[3,1] + values[4,1] + values[5,1] + values[6,1] + values[7,1])
# class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[2,4] + values[2,5] + values[2,6] + values[2,7] + values[0,2] + values[1,2] + values[3,2] + values[4,2] + values[5,2] + values[6,2] + values[7,2])
# class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[3,4] + values[3,5] + values[3,6] + values[3,7] + values[0,3] + values[1,3] + values[2,3] + values[4,3] + values[5,3] + values[6,3] + values[7,3])
# class5_IoU = values[4,4]/(values[4,4] + values[4,0] + values[4,1] + values[4,2] + values[4,3] + values[4,5] + values[4,6] + values[4,7] + values[0,4] + values[1,4] + values[2,4] + values[3,4] + values[5,4] + values[6,4] + values[7,4])
# class6_IoU = values[5,5]/(values[5,5] + values[5,0] + values[5,1] + values[5,2] + values[5,3] + values[5,4] + values[5,6] + values[5,7] + values[0,5] + values[1,5] + values[2,5] + values[3,5] + values[4,5] + values[6,5] + values[7,5])
# class7_IoU = values[6,6]/(values[6,6] + values[6,0] + values[6,1] + values[6,2] + values[6,3] + values[6,4] + values[6,5] + values[6,7] + values[0,6] + values[1,6] + values[2,6] + values[3,6] + values[4,6] + values[5,6] + values[7,6])
# class8_IoU = values[7,7]/(values[7,7] + values[7,0] + values[7,1] + values[7,2] + values[7,3] + values[7,4] + values[7,5] + values[7,6] + values[0,7] + values[1,7] + values[2,7] + values[3,7] + values[4,7] + values[5,7] + values[6,7])
class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[0,4] + values[0,5] + values[0,6] + values[1,0] + values[2,0] + values[3,0] + values[4,0] + values[5,0] + values[6,0])
class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[1,4] + values[1,5] + values[1,6] + values[0,1] + values[2,1] + values[3,1] + values[4,1] + values[5,1] + values[6,1])
class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[2,4] + values[2,5] + values[2,6] + values[0,2] + values[1,2] + values[3,2] + values[4,2] + values[5,2] + values[6,2])
class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[3,4] + values[3,5] + values[3,6] + values[0,3] + values[1,3] + values[2,3] + values[4,3] + values[5,3] + values[6,3])
class5_IoU = values[4,4]/(values[4,4] + values[4,0] + values[4,1] + values[4,2] + values[4,3] + values[4,5] + values[4,6] + values[0,4] + values[1,4] + values[2,4] + values[3,4] + values[5,4] + values[6,4])
class6_IoU = values[5,5]/(values[5,5] + values[5,0] + values[5,1] + values[5,2] + values[5,3] + values[5,4] + values[5,6] + values[0,5] + values[1,5] + values[2,5] + values[3,5] + values[4,5] + values[6,5])
class7_IoU = values[6,6]/(values[6,6] + values[6,0] + values[6,1] + values[6,2] + values[6,3] + values[6,4] + values[6,5] + values[0,6] + values[1,6] + values[2,6] + values[3,6] + values[4,6] + values[5,6])
print("IoU for Background is: ", class1_IoU) #Background
print("IoU for Clasts is: ", class2_IoU) #Clasts
print("IoU for Riffle-shallow-glare is: ", class3_IoU) #Riffle-shallow-glare
print("IoU for Low vegetation is: ", class4_IoU) #Low vegetation
print("IoU for Pool is: ", class5_IoU) #Pool
print("IoU for Riparian vegetation is: ", class6_IoU) #Riparian vegetation
print("IoU for Bar is: ", class7_IoU) #Shadow
# Sat, 17 Jul 2021 22:16:52
from patchify import patchify, unpatchify
large_image = cv2.imread('/media/jose/datos/deep_learning/geomorfologia-detalle-tramo-1km-rio-mana/machine_learning_rf/images/original/mana_whole_padded.tif')
STEP = SIZE_X//2
pad_width = ((SIZE_X, SIZE_Y), (SIZE_X, SIZE_Y), (0, 0))
unpad_width = ((STEP*3//2, STEP*3//2), (STEP*3//2, STEP*3//2))
large_image = np.pad(large_image, pad_width)
patches = patchify(large_image, (SIZE_X, SIZE_Y, large_image.shape[2]), step=STEP)[:, :, 0, :, :, :]
del large_image
gc.collect()
#[Out]# 0
# Sat, 17 Jul 2021 22:17:00
predicted_patches = []
for i in range(patches.shape[0]):
    for j in range(patches.shape[1]):
        print(i,j)
        single_patch = patches[i,j,:,:,:]
        non_zeros = np.count_nonzero(single_patch)
        if(non_zeros>0):
            single_patch_norm = normalize(np.array(single_patch), axis=2)
            single_patch_input=np.expand_dims(single_patch_norm, 0)
            single_patch_prediction = (model.predict(single_patch_input))
            single_patch_predicted_img=np.argmax(single_patch_prediction, axis=3)[0,:,:]
        else:
            single_patch_predicted_img=np.zeros(shape=single_patch.shape[:2], dtype='int64')
        predicted_patches.append(single_patch_predicted_img)
os.system('echo "Job finished" | mail -s "Job finished" zoneminderjr@gmail.com')
#[Out]# 0
# Sat, 17 Jul 2021 22:19:55
predicted_patches = np.array(predicted_patches)
predicted_patches_reshaped = np.reshape(predicted_patches, patches.shape[:4])
del predicted_patches
gc.collect()
predicted_patches_cropped = cp.crop_patches(predicted_patches_reshaped, crop_size=STEP)
del predicted_patches_reshaped
gc.collect()
reconstructed_image = unpatchify(predicted_patches_cropped, (predicted_patches_cropped.shape[0]*STEP, predicted_patches_cropped.shape[1]*STEP))
reconstructed_image = unpad(reconstructed_image, unpad_width)
cv2.imwrite(path_global + 'images/predict/segmented_whole_image/predicted.tif', reconstructed_image)
#[Out]# True
